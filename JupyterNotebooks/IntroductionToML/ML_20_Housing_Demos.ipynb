{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5337e65",
   "metadata": {},
   "source": [
    "\n",
    "# 20 Hands-on ML Programs (House-Price Style Demos)\n",
    "**Focus:** Practical regression workflows like predicting house prices  \n",
    "**Stack:** Python, scikit-learn, NumPy, pandas, matplotlib (offline-friendly)  \n",
    "**Last updated:** 2025-09-05 17:42\n",
    "\n",
    "These 20 small programs showcase end-to-end ML tasks you can adapt for real projects:\n",
    "- Clean *inputs → pipeline → model → metrics → sample predictions*\n",
    "- Concepts: preprocessing, encoding, scaling, model zoo, regularization, CV, grid search, feature selection, robustness, learning curves, persistence.\n",
    "\n",
    "> **Note:** We use **synthetic but realistic housing-like data** (e.g., area, bedrooms, location score, city, parking) so you can run offline without external downloads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045cb577",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Core imports and environment check\n",
    "import sys, warnings, math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, learning_curve\n",
    "    from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor, RANSACRegressor\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.feature_selection import SelectKBest, f_regression\n",
    "    import joblib\n",
    "    SKLEARN_OK = True\n",
    "except Exception as e:\n",
    "    SKLEARN_OK = False\n",
    "    print(\"⚠️ scikit-learn or dependencies not available:\", e)\n",
    "    print(\"Please ensure scikit-learn, numpy, pandas, matplotlib are installed in this environment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e8ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Utility functions: synthetic housing-like dataset + evaluation helpers\n",
    "\n",
    "def generate_housing(n=800, seed=42, with_cats=True, with_noise=True):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # Numeric features\n",
    "    area = rng.normal(1000, 300, n).clip(350, 3000)                 # sqft\n",
    "    bedrooms = rng.integers(1, 5, n)                                # 1..4\n",
    "    age = rng.integers(0, 35, n)                                    # years\n",
    "    floor = rng.integers(0, 15, n)                                  # floor number\n",
    "    loc_score = rng.integers(1, 6, n)                               # 1..5\n",
    "    distance_center = rng.normal(8, 5, n).clip(0.5, 25)             # km\n",
    "    \n",
    "    # Categorical features\n",
    "    if with_cats:\n",
    "        city = rng.choice(['Bengaluru','Mumbai','Delhi','Chennai'], n, p=[0.4,0.25,0.2,0.15])\n",
    "        parking = rng.choice(['none','street','garage'], n, p=[0.2,0.5,0.3])\n",
    "        balcony = rng.integers(0, 3, n)                              # 0..2 (could treat as numeric)\n",
    "    else:\n",
    "        city = np.array(['Bengaluru']*n)\n",
    "        parking = np.array(['street']*n)\n",
    "        balcony = rng.integers(0, 3, n)\n",
    "    \n",
    "    # Price generation (non-linear + interactions) in lakhs\n",
    "    base = (\n",
    "        0.07*area\n",
    "        + 8.0*bedrooms\n",
    "        - 0.9*age\n",
    "        + 2.0*loc_score\n",
    "        - 1.5*distance_center\n",
    "        + 1.0*floor\n",
    "        + 3.0*balcony\n",
    "    )\n",
    "    # City effect\n",
    "    city_adj = {'Bengaluru': 15, 'Mumbai': 35, 'Delhi': 25, 'Chennai': 18}\n",
    "    city_bias = np.vectorize(city_adj.get)(city)\n",
    "    # Parking effect\n",
    "    pk_adj = {'none': 0, 'street': 6, 'garage': 12}\n",
    "    pk_bias = np.vectorize(pk_adj.get)(parking)\n",
    "    \n",
    "    price = base + city_bias + pk_bias\n",
    "    # Non-linear bump: diminishing returns on very large area\n",
    "    price -= 0.000006*(area-1500).clip(0)**2\n",
    "    # Noise\n",
    "    if with_noise:\n",
    "        price += rng.normal(0, 10, n)  # observation noise\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"area\": area.round(0),\n",
    "        \"bedrooms\": bedrooms,\n",
    "        \"age\": age,\n",
    "        \"floor\": floor,\n",
    "        \"loc_score\": loc_score,\n",
    "        \"distance_center\": distance_center.round(2),\n",
    "        \"balcony\": balcony,\n",
    "        \"city\": city,\n",
    "        \"parking\": parking,\n",
    "        \"price\": price.round(2),\n",
    "    })\n",
    "    return df\n",
    "\n",
    "def eval_regression(y_true, y_pred, label=\"Model\"):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{label} -> MAE: {mae:.2f} | RMSE: {rmse:.2f} | R²: {r2:.3f}\")\n",
    "    return mae, rmse, r2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d6c553",
   "metadata": {},
   "source": [
    "## Dataset Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f76af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SKLEARN_OK:\n",
    "    df = generate_housing(n=12, seed=7)\n",
    "    display(df.head(10))\n",
    "else:\n",
    "    print(\"Install scikit-learn, numpy, pandas to run the demos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d295a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Common train/test split + feature lists\n",
    "if SKLEARN_OK:\n",
    "    data = generate_housing(n=1000, seed=10)\n",
    "    num_features = [\"area\",\"bedrooms\",\"age\",\"floor\",\"loc_score\",\"distance_center\",\"balcony\"]\n",
    "    cat_features = [\"city\",\"parking\"]\n",
    "    X = data[num_features + cat_features]\n",
    "    y = data[\"price\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "    # Shared preprocessors\n",
    "    numeric_pipe = Pipeline([\n",
    "        (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "        # scaling optional per model; enable in pipelines where needed\n",
    "    ])\n",
    "    categorical_pipe = Pipeline([\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "    preproc = ColumnTransformer([\n",
    "        (\"num\", numeric_pipe, num_features),\n",
    "        (\"cat\", categorical_pipe, cat_features),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1255c4",
   "metadata": {},
   "source": [
    "### 1) Baseline Linear Regression (numeric-only features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d7135",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SKLEARN_OK:\n",
    "    Xn = X_train[num_features]; Xt = X_test[num_features]\n",
    "    pipe = Pipeline([\n",
    "        (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"lin\", LinearRegression())\n",
    "    ])\n",
    "    pipe.fit(Xn, y_train)\n",
    "    preds = pipe.predict(Xt)\n",
    "    eval_regression(y_test, preds, \"LinearRegression (numeric-only)\")\n",
    "    print(\"Sample preds:\", np.round(preds[:5], 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d5380",
   "metadata": {},
   "source": [
    "### 2) Linear Regression with One-Hot Encoding (full features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209b72c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SKLEARN_OK:\n",
    "    pipe = Pipeline([\n",
    "        (\"pre\", preproc),\n",
    "        (\"lin\", LinearRegression())\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_test)\n",
    "    eval_regression(y_test, preds, \"LinearRegression + OneHot\")\n",
    "    print(\"Sample preds:\", np.round(preds[:5], 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef349d0",
   "metadata": {},
   "source": [
    "### 3) Polynomial Features + Linear Regression (captures simple non-linearities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eccaf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SKLEARN_OK:\n",
    "    poly_cols = [\"area\",\"distance_center\",\"loc_score\",\"age\"]\n",
    "    poly_prep = ColumnTransformer([\n",
    "        (\"poly\", Pipeline([\n",
    "            (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"poly\", PolynomialFeatures(degree=2, include_bias=False))\n",
    "        ]), poly_cols),\n",
    "        (\"rest_num\", numeric_pipe, [c for c in num_features if c not in poly_cols]),\n",
    "        (\"cat\", categorical_pipe, cat_features),\n",
    "    ])\n",
    "    pipe = Pipeline([\n",
    "        (\"pre\", poly_prep),\n",
    "        (\"lin\", LinearRegression())\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_test)\n",
    "    eval_regression(y_test, preds, \"Poly(2)+LinearRegression\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de639e8c",
   "metadata": {},
   "source": [
    "### 4) Ridge Regression (with StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85903108",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SKLEARN_OK:\n",
    "    ridge_pipe = Pipeline([\n",
    "        (\"pre\", ColumnTransformer([\n",
    "            (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())]), num_features),\n",
    "            (\"cat\", categorical_pipe, cat_features),\n",
    "        ])),\n",
    "        (\"ridge\", Ridge(alpha=10.0, random_state=0))\n",
    "    ])\n",
    "    ridge_pipe.fit(X_train, y_train)\n",
    "    preds = ridge_pipe.predict(X_test)\n",
    "    eval_regression(y_test, preds, \"Ridge(alpha=10)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93439cdd",
   "metadata": {},
   "source": [
    "### 5) Lasso Regression (sparse weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f91b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SKLEARN_OK:\n",
    "    lasso_pipe = Pipeline([\n",
    "        (\"pre\", ColumnTransformer([\n",
    "            (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())]), num_features),\n",
    "            (\"cat\", categorical_pipe, cat_features),\n",
    "        ])),\n",
    "        (\"lasso\", Lasso(alpha=0.01, random_state=0, max_iter=10000))\n",
    "    ])\n",
    "    lasso_pipe.fit(X_train, y_train)\n",
    "    preds = lasso_pipe.predict(X_test)\n",
    "    eval_regression(y_test, preds, \"Lasso(alpha=0.01)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5d7a93",
   "metadata": {},
   "source": [
    "### 6) ElasticNet (L1+L2 blend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44a1b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SKLEARN_OK:\n",
    "    enet_pipe = Pipeline([\n",
    "        (\"pre\", ColumnTransformer([\n",
    "            (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())]), num_features),\n",
    "            (\"cat\", categorical_pipe, cat_features),\n",
    "        ])),\n",
    "        (\"enet\", ElasticNet(alpha=0.02, l1_ratio=0.5, random_state=0, max_iter=10000))\n",
    "    ])\n",
    "    enet_pipe.fit(X_train, y_train)\n",
    "    preds = enet_pipe.predict(X_test)\n",
    "    eval_regression(y_test, preds, \"ElasticNet(alpha=0.02, l1_ratio=0.5)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f860b17",
   "metadata": {},
   "source": [
    "### 7) Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa70c9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SKLEARN_OK:\n",
    "    tree_pipe = Pipeline([\n",
    "        (\"pre\", preproc),\n",
    "        (\"tree\", DecisionTreeRegressor(max_depth=6, random_state=0))\n",
    "    ])\n",
    "    tree_pipe.fit(X_train, y_train)\n",
    "    preds = tree_pipe.predict(X_test)\n",
    "    eval_regression(y_test, preds, \"DecisionTree(max_depth=6)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac9171c",
   "metadata": {},
   "source": [
    "### 8) Random Forest Regressor (feature importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00262ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SKLEARN_OK:\n",
    "    rf = Pipeline([\n",
    "        (\"pre\", preproc),\n",
    "        (\"rf\", RandomForestRegressor(n_estimators=200, random_state=0))\n",
    "    ])\n",
    "    rf.fit(X_train, y_train)\n",
    "    preds = rf.predict(X_test)\n",
    "    eval_regression(y_test, preds, \"RandomForest(200)\")\n",
    "\n",
    "    # Rough feature names from ColumnTransformer\n",
    "    ohe = rf.named_steps[\"pre\"].named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "    cat_names = list(ohe.get_feature_names_out(cat_features))\n",
    "    final_feat_names = num_features + cat_names\n",
    "    importances = rf.named_steps[\"rf\"].feature_importances_\n",
    "    imp = sorted(zip(final_feat_names, importances), key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(\"Top 10 feature importances:\", imp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9a8b45",
   "metadata": {},
   "source": [
    "### 9) Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd4a337",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SKLEARN_OK:\n",
    "    gbr = Pipeline([\n",
    "        (\"pre\", preproc),\n",
    "        (\"gbr\", GradientBoostingRegressor(random_state=0))\n",
    "    ])\n",
    "    gbr.fit(X_train, y_train)\n",
    "    preds = gbr.predict(X_test)\n",
    "    eval_regression(y_test, preds, \"GradientBoosting\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d02c59",
   "metadata": {},
   "source": [
    "### 10) HistGradientBoostingRegressor (fast boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7654da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SKLEARN_OK:\n",
    "    hgb = Pipeline([\n",
    "        (\"pre\", preproc),\n",
    "        (\"hgb\", HistGradientBoostingRegressor(random_state=0))\n",
    "    ])\n",
    "    hgb.fit(X_train, y_train)\n",
    "    preds = hgb.predict(X_test)\n",
    "    eval_regression(y_test, preds, \"HistGradientBoosting\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1645ce3b",
   "metadata": {},
   "source": [
    "### 11) KNN Regressor (with scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a66fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SKLEARN_OK:\n",
    "    knn_pipe = Pipeline([\n",
    "        (\"pre\", ColumnTransformer([\n",
    "            (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())]), num_features),\n",
    "            (\"cat\", categorical_pipe, cat_features),\n",
    "        ])),\n",
    "        (\"knn\", KNeighborsRegressor(n_neighbors=7))\n",
    "    ])\n",
    "    knn_pipe.fit(X_train, y_train)\n",
    "    preds = knn_pipe.predict(X_test)\n",
    "    eval_regression(y_test, preds, \"KNN(k=7)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6f200c",
   "metadata": {},
   "source": [
    "### 12) Support Vector Regressor (RBF kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd18680",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SKLEARN_OK:\n",
    "    svr_pipe = Pipeline([\n",
    "        (\"pre\", ColumnTransformer([\n",
    "            (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())]), num_features),\n",
    "            (\"cat\", categorical_pipe, cat_features),\n",
    "        ])),\n",
    "        (\"svr\", SVR(C=5, epsilon=0.2, kernel=\"rbf\"))\n",
    "    ])\n",
    "    svr_pipe.fit(X_train, y_train)\n",
    "    preds = svr_pipe.predict(X_test)\n",
    "    eval_regression(y_test, preds, \"SVR(RBF)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ac68cf",
   "metadata": {},
   "source": [
    "### 13) Huber Regressor (robust linear model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b275df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SKLEARN_OK:\n",
    "    huber_pipe = Pipeline([\n",
    "        (\"pre\", ColumnTransformer([\n",
    "            (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())]), num_features),\n",
    "            (\"cat\", categorical_pipe, cat_features),\n",
    "        ])),\n",
    "        (\"huber\", HuberRegressor(epsilon=1.35, max_iter=2000))\n",
    "    ])\n",
    "    huber_pipe.fit(X_train, y_train)\n",
    "    preds = huber_pipe.predict(X_test)\n",
    "    eval_regression(y_test, preds, \"HuberRegressor\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290f853e",
   "metadata": {},
   "source": [
    "### 14) RANSAC Regressor (outlier-resistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aad336",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SKLEARN_OK:\n",
    "    base = Pipeline([\n",
    "        (\"pre\", ColumnTransformer([\n",
    "            (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())]), num_features),\n",
    "            (\"cat\", categorical_pipe, cat_features),\n",
    "        ])),\n",
    "        (\"lin\", LinearRegression())\n",
    "    ])\n",
    "    ransac = RANSACRegressor(base_estimator=LinearRegression(), random_state=0)\n",
    "    # Fit on preprocessed numeric-only for simplicity in RANSAC demo\n",
    "    # (RANSAC expects array; we'll preprocess separately)\n",
    "    pre_only = ColumnTransformer([\n",
    "        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())]), num_features),\n",
    "        (\"cat\", categorical_pipe, cat_features),\n",
    "    ])\n",
    "    Xtr = pre_only.fit_transform(X_train)\n",
    "    Xte = pre_only.transform(X_test)\n",
    "    ransac.fit(Xtr, y_train)\n",
    "    preds = ransac.predict(Xte)\n",
    "    eval_regression(y_test, preds, \"RANSAC(Linear)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d641d487",
   "metadata": {},
   "source": [
    "### 15) Handling Missing Values (SimpleImputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f79eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SKLEARN_OK:\n",
    "    # Inject missing values artificially\n",
    "    X_miss = X.copy()\n",
    "    X_miss.loc[X_miss.sample(frac=0.1, random_state=1).index, \"age\"] = np.nan\n",
    "    Xm_train, Xm_test, ym_train, ym_test = train_test_split(X_miss, y, test_size=0.2, random_state=42)\n",
    "    pipe = Pipeline([\n",
    "        (\"pre\", ColumnTransformer([\n",
    "            (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_features),\n",
    "            (\"cat\", categorical_pipe, cat_features),\n",
    "        ])),\n",
    "        (\"rf\", RandomForestRegressor(n_estimators=150, random_state=0))\n",
    "    ])\n",
    "    pipe.fit(Xm_train, ym_train)\n",
    "    preds = pipe.predict(Xm_test)\n",
    "    eval_regression(ym_test, preds, \"RF with Imputer (10% missing age)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d730ff5",
   "metadata": {},
   "source": [
    "### 16) Feature Selection (SelectKBest + Linear Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fc714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SKLEARN_OK:\n",
    "    prep = ColumnTransformer([\n",
    "        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_features),\n",
    "        (\"cat\", categorical_pipe, cat_features),\n",
    "    ])\n",
    "    pipe = Pipeline([\n",
    "        (\"pre\", prep),\n",
    "        (\"select\", SelectKBest(score_func=f_regression, k=10)),\n",
    "        (\"lin\", LinearRegression())\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_test)\n",
    "    eval_regression(y_test, preds, \"SelectKBest(k=10)+Linear\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daa7280",
   "metadata": {},
   "source": [
    "### 17) Cross-Validation (KFold + RandomForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4505f604",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SKLEARN_OK:\n",
    "    rf = Pipeline([(\"pre\", preproc), (\"rf\", RandomForestRegressor(n_estimators=200, random_state=0))])\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    scores = cross_val_score(rf, X, y, cv=kf, scoring=\"neg_mean_absolute_error\")\n",
    "    print(\"CV MAE (5-fold):\", np.round(-scores, 2), \" | Mean:\", np.round(-scores.mean(), 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8713221",
   "metadata": {},
   "source": [
    "### 18) Hyperparameter Tuning (GridSearchCV on RandomForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01da3388",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SKLEARN_OK:\n",
    "    rf = Pipeline([(\"pre\", preproc), (\"rf\", RandomForestRegressor(random_state=0))])\n",
    "    grid = {\n",
    "        \"rf__n_estimators\": [100, 200],\n",
    "        \"rf__max_depth\": [None, 10, 15],\n",
    "        \"rf__min_samples_split\": [2, 5]\n",
    "    }\n",
    "    gs = GridSearchCV(rf, grid, cv=3, scoring=\"neg_mean_absolute_error\", n_jobs=None)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(\"Best params:\", gs.best_params_)\n",
    "    preds = gs.predict(X_test)\n",
    "    eval_regression(y_test, preds, \"GridSearchCV-RF (best)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4635c3",
   "metadata": {},
   "source": [
    "### 19) Learning Curve (Linear Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b0d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SKLEARN_OK:\n",
    "    lin = Pipeline([(\"pre\", preproc), (\"lin\", LinearRegression())])\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        lin, X, y, cv=5, train_sizes=np.linspace(0.1, 1.0, 5), scoring=\"r2\", random_state=None\n",
    "    )\n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, train_scores.mean(axis=1), marker=\"o\", label=\"Train R²\")\n",
    "    plt.plot(train_sizes, val_scores.mean(axis=1), marker=\"s\", label=\"CV R²\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"R² score\")\n",
    "    plt.title(\"Learning Curve: Linear Regression\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c588bab9",
   "metadata": {},
   "source": [
    "### 20) Save & Load a Trained Model (joblib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86849ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SKLEARN_OK:\n",
    "    final_model = Pipeline([(\"pre\", preproc), (\"gbr\", GradientBoostingRegressor(random_state=0))])\n",
    "    final_model.fit(X_train, y_train)\n",
    "    joblib.dump(final_model, \"/mnt/data/house_price_model.joblib\")\n",
    "    print(\"Saved model to /mnt/data/house_price_model.joblib\")\n",
    "    # Load and predict\n",
    "    loaded = joblib.load(\"/mnt/data/house_price_model.joblib\")\n",
    "    sample = X_test.iloc[:3]\n",
    "    print(\"Loaded model predictions:\", np.round(loaded.predict(sample), 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bd25c5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## How to Use This Notebook\n",
    "- Run cells top-to-bottom.\n",
    "- Each program is standalone and prints metrics (MAE, RMSE, R²) and sometimes extra info (feature importances, plots, CV scores).\n",
    "- Swap `seed` or tweak feature weights inside `generate_housing()` to simulate other cities/markets.\n",
    "\n",
    "**Next ideas:** try stacking/ensembling, monotonic constraints (for boosting), partial dependence plots, or exporting to ONNX for deployment.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
