{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f223700",
   "metadata": {},
   "source": [
    "\n",
    "# Practical Machine Learning Tips for Beginners\n",
    "**Focus:** Tools (scikit-learn, Kaggle) • Free learning resources • Ready-to-run templates  \n",
    "**Last updated:** 2025-09-05 17:57  \n",
    "\n",
    "This notebook is a quick-start companion: concise checklists, pitfalls to avoid, and **offline-runnable** code templates using scikit-learn. It also explains **how to use Kaggle** for datasets and practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66056e7f",
   "metadata": {},
   "source": [
    "\n",
    "## Who is this for?\n",
    "- Students and professionals beginning ML.\n",
    "- Anyone who wants a practical, **copy‑paste‑friendly** workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be134b7c",
   "metadata": {},
   "source": [
    "\n",
    "## Quick Start: Setup (Local or Cloud)\n",
    "- **Python**: 3.9+ (3.11+ recommended)\n",
    "- **Install**: `pip install scikit-learn pandas numpy matplotlib jupyter`\n",
    "- **Editor**: VS Code (Python extension) or JupyterLab\n",
    "- **Cloud (free)**: Google Colab, Kaggle Notebooks (no install), or local Anaconda\n",
    "- **Version control**: Git + GitHub (optional but highly recommended)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853675a7",
   "metadata": {},
   "source": [
    "\n",
    "## Golden Rules (Pin this!)\n",
    "1. **Split your data** before looking at metrics (train/validation/test).\n",
    "2. Start with a **simple baseline** (linear/logistic regression).\n",
    "3. Use **pipelines** for preprocessing → model (no data leakage).\n",
    "4. Tune via **cross‑validation**; report mean ± std.\n",
    "5. **Fix random_state** for reproducibility.\n",
    "6. Pick the **right metric** (MAE/RMSE/R² vs. accuracy/F1/AUC).\n",
    "7. Track experiments (even a spreadsheet works).\n",
    "8. Keep a **hold‑out test set** for the final unbiased check.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f2f33",
   "metadata": {},
   "source": [
    "\n",
    "## Common Pitfalls (and how to avoid them)\n",
    "- **Data leakage**: Scaling/encoding fit on the full dataset → Do it **only on train** inside a `Pipeline`/`ColumnTransformer`.\n",
    "- **Overfitting**: Model too complex → cross‑validate, regularize, add data, simplify features.\n",
    "- **Wrong split**: Shuffling time‑series → use time‑based split.\n",
    "- **Imbalanced classes** (classification) → use stratified splits, proper metrics (ROC‑AUC, PR‑AUC, F1), resampling.\n",
    "- **Chasing the metric**: Tune hyperparams on test set → keep test set untouched.\n",
    "- **No baseline**: Always compare to something simple.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a889cc",
   "metadata": {},
   "source": [
    "\n",
    "## Tools You Should Know\n",
    "- **scikit-learn**: classical ML library with clean APIs (models, metrics, pipelines).\n",
    "- **pandas** & **NumPy**: data handling and array math.\n",
    "- **matplotlib**: simple plotting.\n",
    "- **Kaggle**: datasets, notebooks, and competitions (practice ecosystem).\n",
    "- **Jupyter/Colab**: iterate quickly, share notebooks.\n",
    "- **(Optional)** MLflow or Weights & Biases for experiment tracking (free tiers).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dea31c0",
   "metadata": {},
   "source": [
    "\n",
    "## Kaggle: How to Use (Datasets & Notebooks)\n",
    "**Option A: Kaggle Notebooks (zero install)**\n",
    "1. Create a Kaggle account → open **Notebooks** → **New Notebook**.\n",
    "2. **Add Data** from the right sidebar (search public datasets).\n",
    "3. Write code; **Run All**; save and share the notebook.\n",
    "\n",
    "**Option B: Kaggle CLI (local)**\n",
    "1. Create an API token: Kaggle → **Account** → **Create New API Token** (downloads `kaggle.json`).\n",
    "2. Place it at `~/.kaggle/kaggle.json` (Linux/Mac) or `%HOMEPATH%\\.kaggle\\kaggle.json` (Windows).\n",
    "3. Install CLI: `pip install kaggle`\n",
    "4. Download data (example):\n",
    "```bash\n",
    "kaggle datasets download -d zillow/zecon   # example dataset\n",
    "unzip zecon.zip -d data/\n",
    "```\n",
    "\n",
    "> Tip: Start with **Kaggle Learn** micro-courses and beginner competitions (e.g., Titanic).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71c999f",
   "metadata": {},
   "source": [
    "\n",
    "## Free Learning Resources (Curated)\n",
    "- **Kaggle Learn** – bite‑sized, hands‑on lessons.\n",
    "- **scikit‑learn User Guide** – practical, concept‑first docs.\n",
    "- **Google ML Crash Course** – quick theory + exercises.\n",
    "- **fast.ai Practical DL** – when you move to deep learning.\n",
    "- **StatQuest (YouTube)** – crystal‑clear explanations of ML math and ideas.\n",
    "- **OpenML / UCI** – classic datasets for practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3ceeb8",
   "metadata": {},
   "source": [
    "## Template 1: Regression Baseline (offline, synthetic data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fda0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Baseline regression with preprocessing and metrics (runs offline)\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Synthetic dataset (like house prices)\n",
    "X, y = make_regression(n_samples=800, n_features=6, noise=25.0, random_state=42)\n",
    "cols = [f\"f{i}\" for i in range(X.shape[1])]\n",
    "df = pd.DataFrame(X, columns=cols); df[\"target\"] = y\n",
    "\n",
    "num_features = cols  # all numeric in this toy example\n",
    "preproc = ColumnTransformer([\n",
    "    (\"num\", Pipeline([(\"impute\", SimpleImputer(strategy=\"median\")), (\"scale\", StandardScaler())]), num_features)\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[num_features], df[\"target\"], test_size=0.2, random_state=0)\n",
    "\n",
    "pipe = Pipeline([(\"pre\", preproc), (\"model\", LinearRegression())])\n",
    "pipe.fit(X_train, y_train)\n",
    "preds = pipe.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, preds)\n",
    "rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "r2 = r2_score(y_test, preds)\n",
    "print(f\"Linear Regression -> MAE: {mae:.2f} | RMSE: {rmse:.2f} | R²: {r2:.3f}\")\n",
    "\n",
    "# Try Ridge for regularization\n",
    "ridge = Pipeline([(\"pre\", preproc), (\"model\", Ridge(alpha=5.0, random_state=0))])\n",
    "ridge.fit(X_train, y_train)\n",
    "preds_r = ridge.predict(X_test)\n",
    "print(f\"Ridge(alpha=5)     -> MAE: {mean_absolute_error(y_test, preds_r):.2f} | RMSE: {mean_squared_error(y_test, preds_r, squared=False):.2f} | R²: {r2_score(y_test, preds_r):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20480225",
   "metadata": {},
   "source": [
    "## Template 2: Classification Baseline (offline, synthetic data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dd9e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=8, n_informative=5, class_sep=1.2, random_state=42)\n",
    "cols = [f\"f{i}\" for i in range(X.shape[1])]\n",
    "df = pd.DataFrame(X, columns=cols); df[\"label\"] = y\n",
    "\n",
    "num = cols\n",
    "preproc = ColumnTransformer([\n",
    "    (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())]), num)\n",
    "])\n",
    "\n",
    "clf = Pipeline([(\"pre\", preproc), (\"logreg\", LogisticRegression(max_iter=2000, random_state=0))])\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[num], df[\"label\"], test_size=0.2, random_state=0, stratify=df[\"label\"])\n",
    "clf.fit(X_train, y_train)\n",
    "probs = clf.predict_proba(X_test)[:,1]\n",
    "preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_test, preds)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_test, preds, average=\"binary\")\n",
    "auc = roc_auc_score(y_test, probs)\n",
    "print(f\"Accuracy: {acc:.3f} | Precision: {prec:.3f} | Recall: {rec:.3f} | F1: {f1:.3f} | AUC: {auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8498982f",
   "metadata": {},
   "source": [
    "## Template 3: Cross-Validation & Grid Search (scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aa2a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Reuse regression data from Template 1 (rerun that cell first if needed)\n",
    "X_train, X_test, y_train, y_test  # noqa: just to hint variable reuse\n",
    "\n",
    "rf = Pipeline([(\"pre\", preproc), (\"rf\", RandomForestRegressor(random_state=0))])\n",
    "grid = {\n",
    "    \"rf__n_estimators\": [100, 200],\n",
    "    \"rf__max_depth\": [None, 10, 15]\n",
    "}\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "gs = GridSearchCV(rf, grid, cv=cv, scoring=\"neg_mean_absolute_error\")\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Best params:\", gs.best_params_)\n",
    "print(\"Best CV MAE:\", -gs.best_score_)\n",
    "print(\"Test MAE:\", __import__(\"sklearn\").metrics.mean_absolute_error(y_test, gs.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f82533",
   "metadata": {},
   "source": [
    "## Template 4: Learning Curve (how much data do I need?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lin = Pipeline([(\"pre\", preproc), (\"lin\", LinearRegression())])\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    lin, df[num_features], df[\"target\"], cv=5, train_sizes=np.linspace(0.1, 1.0, 5), scoring=\"r2\"\n",
    ")\n",
    "plt.figure()\n",
    "plt.plot(train_sizes, train_scores.mean(axis=1), marker=\"o\", label=\"Train R²\")\n",
    "plt.plot(train_sizes, val_scores.mean(axis=1), marker=\"s\", label=\"CV R²\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"R² score\")\n",
    "plt.title(\"Learning Curve: Linear Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7010d7",
   "metadata": {},
   "source": [
    "## Template 5: Save & Load a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97d0a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import joblib\n",
    "joblib.dump(pipe, \"/mnt/data/baseline_regressor.joblib\")\n",
    "loaded = joblib.load(\"/mnt/data/baseline_regressor.joblib\")\n",
    "print(\"Loaded model score (R² on test):\", loaded.score(X_test[num_features], y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fe0706",
   "metadata": {},
   "source": [
    "\n",
    "## 30‑Day Study Plan (1 hour/day)\n",
    "- **Week 1 (Foundations):** Python, NumPy/Pandas, plotting, train/test split, metrics.\n",
    "- **Week 2 (Classic ML):** Linear/Logistic Regression, k‑NN, Decision Trees; build 2 mini projects.\n",
    "- **Week 3 (Pipelines & Tuning):** ColumnTransformer, cross‑validation, grid search; try Kaggle Titanic.\n",
    "- **Week 4 (Practice & Reporting):** One full project: EDA → baseline → tuning → write a 1‑page report with results and lessons learned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc392227",
   "metadata": {},
   "source": [
    "\n",
    "## Starter Project Ideas\n",
    "1. **House prices** (regression) – engineer features like price per sqft, location score.\n",
    "2. **Used car prices** (regression) – brand, year, mileage, fuel, transmission.\n",
    "3. **Loan default** (classification) – credit score, income, age, history.\n",
    "4. **Customer churn** (classification) – usage metrics, tenure, support tickets.\n",
    "5. **Flight delays** (regression/classification) – origin/destination, time, weather proxy.\n",
    "6. **Energy consumption** (regression) – weather, hour, day type (workday/holiday).\n",
    "7. **Student performance** (classification) – study time, attendance, prep courses.\n",
    "8. **Retail sales forecasting** (regression) – promotions, seasonality, store/category.\n",
    "9. **Health risk scoring** (classification) – anonymized vitals, habits (ethics first!).\n",
    "10. **Fraud detection** (classification) – highly imbalanced; focus on precision‑recall.\n",
    "\n",
    "> Use Kaggle/OpenML to find public datasets matching these ideas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8195a419",
   "metadata": {},
   "source": [
    "\n",
    "## Responsible ML (Always)\n",
    "- Respect privacy; minimize sensitive data.\n",
    "- Check for **bias** and disparate impact.\n",
    "- Keep an **audit trail**: data source, preprocessing, model version, metrics.\n",
    "- Prefer **explainable** models when decisions affect people.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f09e93d",
   "metadata": {},
   "source": [
    "\n",
    "## Your Personal Checklist (fill these in)\n",
    "- Goal/target variable:\n",
    "- Primary metric:\n",
    "- Baseline result:\n",
    "- Best model (so far):\n",
    "- What went wrong? (top 3 issues)\n",
    "- Next experiments:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
