{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6b69a3c",
   "metadata": {},
   "source": [
    "\n",
    "# 20 Hands‑On Mini‑Programs — Core Machine Learning Concepts\n",
    "\n",
    "This notebook gives you **20 tiny, runnable programs** that explain the foundations of ML using **friendly analogies** and **real‑world mini examples**.  \n",
    "We keep it dependency‑light (pure Python), so you can run everything anywhere.\n",
    "\n",
    "**What you'll practice:** data → features → labels → models → evaluation → overfitting → good habits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0142929",
   "metadata": {},
   "source": [
    "## Shared Utilities (used by multiple programs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3c7690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random, math, statistics\n",
    "random.seed(42)\n",
    "\n",
    "# --- Metrics ---\n",
    "def accuracy(y_true, y_pred):\n",
    "    return sum(1 for a,b in zip(y_true, y_pred) if a==b) / max(1,len(y_true))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return sum(abs(a-b) for a,b in zip(y_true, y_pred)) / max(1,len(y_true))\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return sum((a-b)**2 for a,b in zip(y_true, y_pred)) / max(1,len(y_true))\n",
    "\n",
    "def precision_recall(y_true, y_pred, positive=1):\n",
    "    tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt==positive and yp==positive)\n",
    "    fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt!=positive and yp==positive)\n",
    "    fn = sum(1 for yt, yp in zip(y_true, y_pred) if yt==positive and yp!=positive)\n",
    "    prec = tp / (tp+fp) if (tp+fp)>0 else 0.0\n",
    "    rec  = tp / (tp+fn) if (tp+fn)>0 else 0.0\n",
    "    return prec, rec\n",
    "\n",
    "# --- Splits ---\n",
    "def train_test_split(data, test_ratio=0.3, seed=42):\n",
    "    rnd = random.Random(seed)\n",
    "    X = data[:]\n",
    "    rnd.shuffle(X)\n",
    "    cut = int(len(X)*(1-test_ratio))\n",
    "    return X[:cut], X[cut:]\n",
    "\n",
    "# --- Feature scaling (min-max) ---\n",
    "def fit_minmax(train, feature_names):\n",
    "    params = {}\n",
    "    for f in feature_names:\n",
    "        vals = [row[f] for row in train if row[f] is not None]\n",
    "        mn = min(vals); mx = max(vals)\n",
    "        params[f] = (mn, mx if mx>mn else mn+1e-9)\n",
    "    return params\n",
    "\n",
    "def apply_minmax(rows, feature_names, params):\n",
    "    out = []\n",
    "    for r in rows:\n",
    "        nr = r.copy()\n",
    "        for f in feature_names:\n",
    "            mn, mx = params[f]\n",
    "            nr[f] = (r[f]-mn)/(mx-mn)\n",
    "        out.append(nr)\n",
    "    return out\n",
    "\n",
    "# --- Simple k-NN (from scratch) ---\n",
    "def euclidean(a, b, feature_names):\n",
    "    return math.sqrt(sum((a[f]-b[f])**2 for f in feature_names))\n",
    "\n",
    "def knn_classify(train, x, k, feature_names, label_name):\n",
    "    dists = [(euclidean(row, x, feature_names), row[label_name]) for row in train]\n",
    "    dists.sort(key=lambda t: t[0])\n",
    "    top = [lab for _,lab in dists[:k]]\n",
    "    # majority vote\n",
    "    return max(set(top), key=top.count)\n",
    "\n",
    "def knn_regress(train, x, k, feature_names, target_name):\n",
    "    dists = [(euclidean(row, x, feature_names), row[target_name]) for row in train]\n",
    "    dists.sort(key=lambda t: t[0])\n",
    "    top = [val for _,val in dists[:k]]\n",
    "    return sum(top)/len(top)\n",
    "\n",
    "# --- Gradient Descent for Univariate Linear Regression with L2 ---\n",
    "def linreg_univariate_gd(X, y, lr=0.01, epochs=1000, l2=0.0):\n",
    "    # Model: y_hat = w*x + b\n",
    "    w = 0.0; b = 0.0\n",
    "    n = len(X)\n",
    "    for _ in range(epochs):\n",
    "        y_hat = [w*x + b for x in X]\n",
    "        # gradients\n",
    "        dw = (2/n)*sum((yh - yt)*x for yh,yt,x in zip(y_hat, y, X)) + 2*l2*w\n",
    "        db = (2/n)*sum((yh - yt) for yh,yt in zip(y_hat, y))\n",
    "        w -= lr*dw\n",
    "        b -= lr*db\n",
    "    return w, b\n",
    "\n",
    "# --- One-hot encoding for a single categorical feature ---\n",
    "def fit_one_hot(values):\n",
    "    uniq = sorted(set(values))\n",
    "    index = {v:i for i,v in enumerate(uniq)}\n",
    "    return index\n",
    "\n",
    "def transform_one_hot(value, index):\n",
    "    vec = [0]*len(index)\n",
    "    vec[index[value]] = 1\n",
    "    return vec\n",
    "\n",
    "# --- Simple decision stump (one feature, best threshold by error) ---\n",
    "def decision_stump_fit(rows, feature, label):\n",
    "    # consider midpoints between sorted unique feature values\n",
    "    arr = sorted((r[feature], r[label]) for r in rows)\n",
    "    candidates = []\n",
    "    for i in range(len(arr)-1):\n",
    "        a, _ = arr[i]; b, _ = arr[i+1]\n",
    "        if a != b:\n",
    "            candidates.append((a+b)/2)\n",
    "    best = (float(\"inf\"), None, None)  # error, threshold, polarity\n",
    "    for thr in candidates:\n",
    "        for polarity in [1, -1]:  # 1: predict 1 if x>=thr else 0; -1: reverse\n",
    "            errs = 0\n",
    "            for x, y in arr:\n",
    "                pred = 1 if (x>=thr) else 0\n",
    "                if polarity==-1: pred = 1-pred\n",
    "                errs += (pred!=y)\n",
    "            if errs < best[0]:\n",
    "                best = (errs, thr, polarity)\n",
    "    return {\"threshold\": best[1], \"polarity\": best[2]}\n",
    "\n",
    "def decision_stump_predict(model, x):\n",
    "    thr = model[\"threshold\"]; pol = model[\"polarity\"]\n",
    "    pred = 1 if (x>=thr) else 0\n",
    "    if pol==-1: pred = 1-pred\n",
    "    return pred\n",
    "\n",
    "# --- Imputation ---\n",
    "def mean_impute(rows, feature):\n",
    "    vals = [r[feature] for r in rows if r[feature] is not None]\n",
    "    m = sum(vals)/len(vals)\n",
    "    for r in rows:\n",
    "        if r[feature] is None:\n",
    "            r[feature] = m\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c81960b",
   "metadata": {},
   "source": [
    "### Program 1 — Data, Features, Labels (Fruit Ripeness Analogy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea62a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data as a list of dicts; features: weight (g), color_intensity (0-1); label: ripe (1) or not (0)\n",
    "data = [\n",
    "    {\"weight\": 120, \"color\": 0.2, \"ripe\": 0},\n",
    "    {\"weight\": 150, \"color\": 0.6, \"ripe\": 1},\n",
    "    {\"weight\": 135, \"color\": 0.55, \"ripe\": 1},\n",
    "    {\"weight\": 90,  \"color\": 0.1, \"ripe\": 0},\n",
    "]\n",
    "features = [\"weight\",\"color\"]\n",
    "labels = [row[\"ripe\"] for row in data]\n",
    "print(\"Features names:\", features)\n",
    "print(\"First row features:\", {f:data[0][f] for f in features}, \"Label:\", data[0][\"ripe\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46789710",
   "metadata": {},
   "source": [
    "### Program 2 — Turning Raw Text into Features (Support Ticket Urgency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bff7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tickets = [\n",
    "    {\"msg\":\"Server down ASAP!!!\", \"urgent\":1},\n",
    "    {\"msg\":\"Minor UI bug on profile page\", \"urgent\":0},\n",
    "    {\"msg\":\"Payment failing for multiple users!\", \"urgent\":1},\n",
    "    {\"msg\":\"Typo in footer\", \"urgent\":0},\n",
    "]\n",
    "urgent_words = {\"asap\",\"urgent\",\"failing\",\"down\",\"error\"}\n",
    "\n",
    "def text_features(s):\n",
    "    words = s.lower().split()\n",
    "    return {\n",
    "        \"len\": len(s),\n",
    "        \"bangs\": s.count(\"!\"),\n",
    "        \"urgent_count\": sum(1 for w in words if w.strip(\"!.,\") in urgent_words)\n",
    "    }\n",
    "\n",
    "feats = [{\"features\": text_features(t[\"msg\"]), \"label\": t[\"urgent\"]} for t in tickets]\n",
    "for f in feats:\n",
    "    print(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f5cd27",
   "metadata": {},
   "source": [
    "### Program 3 — Classification vs Regression (Spam vs Delivery Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a52cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "emails = [\n",
    "    {\"words\": 120, \"links\": 5, \"spam\":1},\n",
    "    {\"words\": 30,  \"links\": 0, \"spam\":0},\n",
    "]\n",
    "deliveries = [\n",
    "    {\"distance_km\": 2.0, \"time_min\": 9.0},\n",
    "    {\"distance_km\": 6.0, \"time_min\": 22.0},\n",
    "]\n",
    "\n",
    "print(\"Classification label (spam):\", [e[\"spam\"] for e in emails])\n",
    "print(\"Regression target (delivery time):\", [d[\"time_min\"] for d in deliveries])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46d0a92",
   "metadata": {},
   "source": [
    "### Program 4 — Train/Test Split + Baseline (Majority Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c2fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = [{\"x\":i, \"label\": 1 if i%3==0 else 0} for i in range(30)]  # more zeros than ones\n",
    "train, test = train_test_split(dataset, test_ratio=0.3, seed=7)\n",
    "majority = 1 if sum(r[\"label\"] for r in train) > (len(train)/2) else 0\n",
    "y_true = [r[\"label\"] for r in test]\n",
    "y_pred = [majority]*len(test)\n",
    "print(\"Baseline majority prediction:\", majority)\n",
    "print(\"Test accuracy:\", round(accuracy(y_true, y_pred), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9126ec",
   "metadata": {},
   "source": [
    "### Program 5 — k-NN Classifier (Weather → Play Outside?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd93b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weather = [\n",
    "    {\"temp\":28, \"humidity\":60, \"play\":1},\n",
    "    {\"temp\":35, \"humidity\":80, \"play\":0},\n",
    "    {\"temp\":22, \"humidity\":50, \"play\":1},\n",
    "    {\"temp\":31, \"humidity\":65, \"play\":0},\n",
    "    {\"temp\":25, \"humidity\":55, \"play\":1},\n",
    "]\n",
    "features = [\"temp\",\"humidity\"]\n",
    "train, test = train_test_split(weather, test_ratio=0.4, seed=1)\n",
    "k = 3\n",
    "preds = [knn_classify(train, row, k, features, \"play\") for row in test]\n",
    "print(\"True:\", [r[\"play\"] for r in test])\n",
    "print(\"Pred:\", preds, \"Accuracy:\", round(accuracy([r[\"play\"] for r in test], preds), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a6b7d",
   "metadata": {},
   "source": [
    "### Program 6 — k-NN Regressor (House Price by Size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb0cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "houses = [{\"size\": s, \"price\": 50 + 0.3*s + random.uniform(-5,5)} for s in range(600, 1001, 100)]\n",
    "train, test = train_test_split(houses, test_ratio=0.33, seed=2)\n",
    "features = [\"size\"]; target = \"price\"\n",
    "k=3\n",
    "y_true = [r[target] for r in test]\n",
    "y_pred = [knn_regress(train, r, k, features, target) for r in test]\n",
    "print(\"MAE:\", round(mae(y_true, y_pred), 2))\n",
    "for r, p in zip(test, y_pred):\n",
    "    print(f\"size={r['size']}, true={round(r[target],1)}, pred={round(p,1)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b8cd3f",
   "metadata": {},
   "source": [
    "### Program 7 — Linear Regression via Gradient Descent (Ice‑cream Sales vs Temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a43ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Synthetic relation: sales = 10*temp + 5 + noise\n",
    "temps = [18,20,22,24,26,28,30]\n",
    "sales = [10*t + 5 + random.uniform(-10,10) for t in temps]\n",
    "w, b = linreg_univariate_gd(temps, sales, lr=0.001, epochs=5000)\n",
    "preds = [w*t + b for t in temps]\n",
    "print(\"Learned: sales ≈\", round(w,2), \"* temp +\", round(b,2))\n",
    "print(\"MAE:\", round(mae(sales, preds), 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0fa754",
   "metadata": {},
   "source": [
    "### Program 8 — Feature Scaling Matters (k‑NN Distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8773054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "people = [\n",
    "    {\"age\":20, \"income\":20000, \"buy\":0},\n",
    "    {\"age\":21, \"income\":21000, \"buy\":0},\n",
    "    {\"age\":45, \"income\":120000, \"buy\":1},\n",
    "    {\"age\":46, \"income\":125000, \"buy\":1},\n",
    "]\n",
    "features = [\"age\",\"income\"]\n",
    "train, test = people[:3], people[3:]\n",
    "\n",
    "# Without scaling\n",
    "pred_raw = [knn_classify(train, row, 1, features, \"buy\") for row in test]\n",
    "\n",
    "# With min-max scaling using train stats\n",
    "params = fit_minmax(train, features)\n",
    "train_scaled = apply_minmax(train, features, params)\n",
    "test_scaled  = apply_minmax(test,  features, params)\n",
    "pred_scaled = [knn_classify(train_scaled, row, 1, features, \"buy\") for row in test_scaled]\n",
    "\n",
    "print(\"Prediction w/o scaling:\", pred_raw)\n",
    "print(\"Prediction with scaling:\", pred_scaled, \"(often better because distances are balanced)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f4b6a3",
   "metadata": {},
   "source": [
    "### Program 9 — Overfitting: k=1 vs k=7 (Noisy Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa47e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build noisy 1D classification: label = 1 if x>0.5, but flip 15% randomly\n",
    "data = [{\"x\":random.random(), \"y\": 1 if random.random() > 0.15 else 0} for _ in range(80)]\n",
    "for r in data: \n",
    "    r[\"y\"] = 1 if r[\"x\"]>0.5 else 0\n",
    "    if random.random()<0.15: r[\"y\"]=1-r[\"y\"]\n",
    "train, test = train_test_split(data, test_ratio=0.3, seed=3)\n",
    "feat = [\"x\"]\n",
    "y_true = [r[\"y\"] for r in test]\n",
    "pred_k1 = [knn_classify(train, row, 1, feat, \"y\") for row in test]  # high variance\n",
    "pred_k7 = [knn_classify(train, row, 7, feat, \"y\") for row in test]  # smoother\n",
    "print(\"Acc k=1:\", round(accuracy(y_true, pred_k1),3), \"| Acc k=7:\", round(accuracy(y_true, pred_k7),3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd80f8de",
   "metadata": {},
   "source": [
    "### Program 10 — Cross‑Validation (Pick a Good k for k‑NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d70de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Small dataset: points near (0,0)->class 0, near (1,1)->class 1\n",
    "data = []\n",
    "for _ in range(20):\n",
    "    data.append({\"x\": random.uniform(0,0.3), \"y\": random.uniform(0,0.3), \"c\":0})\n",
    "    data.append({\"x\": random.uniform(0.7,1.0), \"y\": random.uniform(0.7,1.0), \"c\":1})\n",
    "\n",
    "def kfold_indices(n, k=5, seed=123):\n",
    "    idx = list(range(n))\n",
    "    random.Random(seed).shuffle(idx)\n",
    "    fold_size = n//k\n",
    "    folds = [idx[i*fold_size:(i+1)*fold_size] for i in range(k-1)]\n",
    "    folds.append(idx[(k-1)*fold_size:])\n",
    "    return folds\n",
    "\n",
    "def cv_score_k(k_val):\n",
    "    folds = kfold_indices(len(data), k=5)\n",
    "    feats = [\"x\",\"y\"]\n",
    "    scores = []\n",
    "    for i in range(5):\n",
    "        test_idx = set(folds[i])\n",
    "        train_rows = [data[j] for j in range(len(data)) if j not in test_idx]\n",
    "        test_rows  = [data[j] for j in range(len(data)) if j in test_idx]\n",
    "        y_true = [r[\"c\"] for r in test_rows]\n",
    "        y_pred = [knn_classify(train_rows, r, k_val, feats, \"c\") for r in test_rows]\n",
    "        scores.append(accuracy(y_true, y_pred))\n",
    "    return sum(scores)/len(scores)\n",
    "\n",
    "for k in [1,3,5,7,9]:\n",
    "    print(\"k=\",k,\"CV accuracy=\", round(cv_score_k(k),3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec5be9",
   "metadata": {},
   "source": [
    "### Program 11 — Regularization (L2) Reduces Overfitting (Noisy Line Fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e76a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# y = 2*x + 1 + noise. Compare GD with and without L2 (same epochs).\n",
    "X = [i for i in range(20)]\n",
    "y = [2*x + 1 + random.uniform(-3,3) for x in X]\n",
    "w1, b1 = linreg_univariate_gd(X, y, lr=0.01, epochs=1000, l2=0.0)\n",
    "w2, b2 = linreg_univariate_gd(X, y, lr=0.01, epochs=1000, l2=0.1)\n",
    "pred1 = [w1*x+b1 for x in X]\n",
    "pred2 = [w2*x+b2 for x in X]\n",
    "print(\"No L2:    w=\", round(w1,3), \"b=\", round(b1,3), \"MSE=\", round(mse(y,pred1),3))\n",
    "print(\"With L2:  w=\", round(w2,3), \"b=\", round(b2,3), \"MSE=\", round(mse(y,pred2),3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db2a7b2",
   "metadata": {},
   "source": [
    "### Program 12 — One‑Hot Encoding (City → Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602dd4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = [\n",
    "    {\"city\":\"Bengaluru\",\"temp\":28},\n",
    "    {\"city\":\"Mumbai\",\"temp\":31},\n",
    "    {\"city\":\"Chennai\",\"temp\":33},\n",
    "    {\"city\":\"Bengaluru\",\"temp\":29},\n",
    "]\n",
    "index = fit_one_hot([r[\"city\"] for r in rows])\n",
    "encoded = [transform_one_hot(r[\"city\"], index) + [r[\"temp\"]] for r in rows]\n",
    "print(\"Index:\", index)\n",
    "print(\"Encoded rows (one‑hot city + temp):\")\n",
    "for vec in encoded: print(vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2688678c",
   "metadata": {},
   "source": [
    "### Program 13 — Handling Missing Values (Mean Imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f17a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "students = [\n",
    "    {\"hours\": 5.0}, {\"hours\": 6.5}, {\"hours\": None}, {\"hours\": 4.0}, {\"hours\": None}\n",
    "]\n",
    "print(\"Before:\", students)\n",
    "students = mean_impute(students, \"hours\")\n",
    "print(\"After:\", students)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec2763d",
   "metadata": {},
   "source": [
    "### Program 14 — Feature Engineering (Build an Engagement Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55863d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "activity = [\n",
    "  {\"views\": 5, \"minutes\": 10, \"replies\": 0},\n",
    "  {\"views\": 12,\"minutes\": 45, \"replies\": 2},\n",
    "  {\"views\": 2, \"minutes\": 3,  \"replies\": 0},\n",
    "]\n",
    "# A simple, explainable score (weights chosen by common sense)\n",
    "for r in activity:\n",
    "    r[\"engagement\"] = 0.2*r[\"views\"] + 0.7*r[\"minutes\"] + 1.5*r[\"replies\"]\n",
    "print(activity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774fb837",
   "metadata": {},
   "source": [
    "### Program 15 — Naive Bayes (Tiny Spam Filter from Scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a604e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = [\n",
    "    {\"text\":\"win cash now\", \"spam\":1},\n",
    "    {\"text\":\"meeting schedule attached\", \"spam\":0},\n",
    "    {\"text\":\"limited offer cash prize\", \"spam\":1},\n",
    "    {\"text\":\"please review the report\", \"spam\":0},\n",
    "]\n",
    "# Build vocabulary\n",
    "def tokenize(s): return [w.strip(\".,!\").lower() for w in s.split()]\n",
    "vocab = sorted(set(w for row in dataset for w in tokenize(row[\"text\"])))\n",
    "# Train: P(class) and P(word|class) with Laplace smoothing\n",
    "class_counts = {0:0,1:0}\n",
    "word_counts = {0:{w:0 for w in vocab}, 1:{w:0 for w in vocab}}\n",
    "for row in dataset:\n",
    "    c = row[\"spam\"]; class_counts[c]+=1\n",
    "    for w in set(tokenize(row[\"text\"])):\n",
    "        word_counts[c][w]+=1\n",
    "def predict(s):\n",
    "    words = set(tokenize(s))\n",
    "    total = sum(class_counts.values())\n",
    "    logp = {}\n",
    "    for c in [0,1]:\n",
    "        # prior\n",
    "        logp[c] = math.log((class_counts[c]+1)/(total+2))\n",
    "        for w in vocab:\n",
    "            # likelihood\n",
    "            pwc = (word_counts[c][w]+1)/(class_counts[c]+2)\n",
    "            logp[c] += math.log(pwc if (w in words) else (1-pwc))\n",
    "    return 1 if logp[1]>logp[0] else 0\n",
    "tests = [\"cash prize available\", \"attach the schedule\", \"win big offer\", \"please check\"]\n",
    "for s in tests:\n",
    "    print(s, \"=>\", \"SPAM\" if predict(s)==1 else \"HAM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01fb169",
   "metadata": {},
   "source": [
    "### Program 16 — Decision Stump (One‑Rule Classifier: Hours Studied → Pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478889bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = [{\"hours\":h, \"pass\": 1 if h>=6 else 0} for h in [2,3,4,5,6,7,8]]\n",
    "# add noise\n",
    "rows[2][\"pass\"]=1; rows[5][\"pass\"]=0\n",
    "model = decision_stump_fit(rows, \"hours\", \"pass\")\n",
    "print(\"Model:\", model)\n",
    "preds = [decision_stump_predict(model, r[\"hours\"]) for r in rows]\n",
    "print(\"Training accuracy:\", round(accuracy([r[\"pass\"] for r in rows], preds),3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bf4bbd",
   "metadata": {},
   "source": [
    "### Program 17 — Imbalanced Data (Accuracy can mislead; check Precision/Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61caed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fraud detection: only 5% positives\n",
    "y_true = [1 if i<5 else 0 for i in range(100)]\n",
    "# \"Dumb\" model predicts all zeros\n",
    "y_pred = [0]*100\n",
    "acc = accuracy(y_true, y_pred)\n",
    "prec, rec = precision_recall(y_true, y_pred, positive=1)\n",
    "print(\"Accuracy:\", acc, \"| Precision:\", prec, \"| Recall:\", rec, \"(terrible recall for rare frauds)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b75c853",
   "metadata": {},
   "source": [
    "### Program 18 — Data Leakage (Cheating Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1f5483",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If we sneak the true label into features, model appears 'perfect' but it's cheating.\n",
    "train = [{\"x\": i, \"label\": i%2} for i in range(10)]\n",
    "# Bad feature: 'leaky' that equals the label itself\n",
    "for r in train: r[\"leaky\"] = r[\"label\"]\n",
    "def trivial_classifier(row): return row[\"leaky\"]  # 100% on train, 0% useful in real world\n",
    "pred = [trivial_classifier(r) for r in train]\n",
    "print(\"Train accuracy (fake):\", accuracy([r[\"label\"] for r in train], pred), \"← data leakage!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3c8aff",
   "metadata": {},
   "source": [
    "### Program 19 — Data Drift (Compare Feature Means Over Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cd5c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train period vs recent period\n",
    "train_period = [{\"x\": random.gauss(10, 2)} for _ in range(200)]\n",
    "recent      = [{\"x\": random.gauss(12, 2)} for _ in range(200)]\n",
    "mean_train = statistics.mean(r[\"x\"] for r in train_period)\n",
    "mean_recent = statistics.mean(r[\"x\"] for r in recent)\n",
    "print(\"Mean(train)=\", round(mean_train,2), \"Mean(recent)=\", round(mean_recent,2), \"Δ=\", round(mean_recent-mean_train,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401678b4",
   "metadata": {},
   "source": [
    "### Program 20 — End‑to‑End Mini Pipeline (One‑Hot + Scaling + k‑NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de047327",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict if a rider will take a ride again next week based on city (categorical) and trips_this_week (numeric).\n",
    "data = [\n",
    "    {\"city\":\"BLR\",\"trips\":3,\"again\":1},\n",
    "    {\"city\":\"BLR\",\"trips\":0,\"again\":0},\n",
    "    {\"city\":\"MUM\",\"trips\":5,\"again\":1},\n",
    "    {\"city\":\"MUM\",\"trips\":1,\"again\":0},\n",
    "    {\"city\":\"DEL\",\"trips\":2,\"again\":1},\n",
    "    {\"city\":\"DEL\",\"trips\":0,\"again\":0},\n",
    "]\n",
    "# One‑hot city\n",
    "index = fit_one_hot([r[\"city\"] for r in data])\n",
    "def to_features(row):\n",
    "    return {\"c0\": transform_one_hot(row[\"city\"], index)[0] if len(index)>0 else 0,\n",
    "            \"c1\": transform_one_hot(row[\"city\"], index)[1] if len(index)>1 else 0,\n",
    "            \"c2\": transform_one_hot(row[\"city\"], index)[2] if len(index)>2 else 0,\n",
    "            \"trips\": row[\"trips\"],\n",
    "            \"again\": row[\"again\"]}\n",
    "rows = [to_features(r) for r in data]\n",
    "# scale numeric 'trips'\n",
    "params = fit_minmax(rows, [\"trips\"])\n",
    "rows = apply_minmax(rows, [\"trips\"], params)\n",
    "\n",
    "train, test = train_test_split(rows, test_ratio=0.33, seed=5)\n",
    "feature_names = [\"c0\",\"c1\",\"c2\",\"trips\"]\n",
    "preds = [knn_classify(train, r, 3, feature_names, \"again\") for r in test]\n",
    "print(\"True:\", [r[\"again\"] for r in test])\n",
    "print(\"Pred:\", preds, \"Accuracy:\", round(accuracy([r[\"again\"] for r in test], preds),3))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}